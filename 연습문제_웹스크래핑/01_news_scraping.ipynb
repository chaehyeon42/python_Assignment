{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0d2477",
   "metadata": {},
   "source": [
    "1-1. Daum 뉴스기사 제목 스크래핑하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0611f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reqeusts, bs4 import\n",
    "import requests\n",
    "import bs4\n",
    "# BeautifulSoup 클래스 import\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464622a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#다음 경제 뉴스 제목 추출하기\n",
    "#1. 직접 url을 선언할 경우\n",
    "# url = 'https://news.daum.net/economy' \n",
    "\n",
    "req_param = {\n",
    "    'sid': 'economy' #economy 같은 경우 파이썬이 문자열로 인식 하기 때문에 작은따옴표 사용 하여 입력\n",
    "}\n",
    "url = 'https://news.daum.net/{sid}'.format(**req_param) #**은 딕셔너리에 파라미터 전달하기 위해 사용\n",
    "print(url)\n",
    "\n",
    "# 요청 헤더 설정 : 브라우저 정보 (설정하는 이유는 사람처럼 보이게 하기 위함)\n",
    "req_header = {\n",
    "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# requests 의 get() 함수 호출하기 \n",
    "res = requests.get(url, headers=req_header)\n",
    "res.encoding = 'utf-8' #한글 깨짐 방지를 위해 인코딩 설정\n",
    "print(res.status_code) # 200이면 성공\n",
    "print(res.ok)\n",
    "print(type(res))  \n",
    "print(res.text) #응답객체에 들어있는 text 속성 (ex.해당 페이지 소스 보기에서 나오는 text들)\n",
    "\n",
    "# BeautifulSoup 객체 생성  \n",
    "if res.ok:\n",
    "    soup = BeautifulSoup(res.text,'html.parser') # res.text를 분석 가능한 구조(soup)로 변환\n",
    "    print(len(soup.select('a')))\n",
    "   # print(len(soup.select('a div.wrap_thumb'))) #a태그 안 div.wrap_thumb 갯수 나타냄\n",
    "    a_tags = soup.select(\"a.item_newsheadline2[href*='https://v.daum.net/v/']\") # a 태그 중 클래스가 item_newsheadline2이고 href에 특정 주소가 포함된 요소 추출\n",
    "    print(type(a_tags),type(a_tags[0]))\n",
    "    # CSS 선택자를 사용해서 a tag 목록 가져오기\n",
    "    #a_tags = soup.select(\"\")\n",
    "# <a> 태그 리스트 순회하기\n",
    "for a_tag in a_tags:\n",
    "    title = a_tag.text.strip()   # .strip()을 사용하여 제목 앞뒤의 불필요한 공백과 줄바꿈 제거\n",
    "    link = a_tag['href'] # a 태그의 href 속성값(기사 상세 링크) 추출\n",
    "    print(title,link)  \n",
    "    #print(type(a_tag),a_tag) #a_tag타입과 a_tag 자체를 print로 확인\n",
    "# print(soup.select(\"div.sa_text a[href*='mnews/article']\"))\n",
    "else:\n",
    "# 응답(response)이 Error 이면 status code 출력    \n",
    "    print(f'Error code = {res.status_code}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a66a5a",
   "metadata": {},
   "source": [
    "1-2.여러개의 section 중 하나를 선택해서 url에서 뉴스기사의 링크와 제목을 출력하는 코드를 함수로 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5691a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "section_dict = {'홈':'home','기후/환경':'climate','사회':'society','경제':'economy','정치':'politics','국제':'world','IT/과학':'tech','문화':'culture','생활':'life','인물':'people'}\n",
    "\n",
    "\n",
    "def print_news(section_name):\n",
    "    sid = section_dict.get(section_name,'홈') #sid 추출 / 해당 키가 없으면 'home'을 기본값으로 사용\n",
    "    print(sid)\n",
    "    url = f'https://news.daum.net/{sid}' #Daum 뉴스 url\n",
    "    print(f'{section_name} 뉴스 {url}')\n",
    "    req_header = {\n",
    "    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'\n",
    "    } #header 설정\n",
    "    res = requests.get(url, headers=req_header) #requests.get : url과 헤더정보로 서버에 데이터 요청\n",
    "    res.encoding = 'utf-8' #한글 깨짐 방지를 위해 인코딩 설정\n",
    "    if res.ok:  #응답 상태가 200(정상)일 경우에만 실행\n",
    "        soup = BeautifulSoup(res.text,'html.parser') #서버로부터 받은 복잡한 텍스트 형태의 HTML(res.text)을 파이썬이 분석하기 쉬운 객체(soup)로 변환합니다\n",
    "        #print(len(soup.select('a')))  #a라는 css 선택자를 print\n",
    "        #print(len(soup.select('a div.wrap_thumb'))) #a 태그 밑에 div 안에 wrap_thumb속성 지닌 것 갯수 출력\n",
    "        a_tags = soup.select(\"a.item_newsheadline2[href*='https://v.daum.net/v/']\") #a tag 에서 a 클래스 이름을 이용해 특정 주소를 포함한 요소들 추출\n",
    "        print(type(a_tags),type(a_tags[0]))#a_tags 타입 과 a_tags 타입 인덱스 확인 (list 형태로 가져옴-<class 'bs4.element.ResultSet'> <class 'bs4.element.Tag'>)\n",
    "# <a> 태그 리스트 순회하기\n",
    "    for a_tag in a_tags:\n",
    "        title = a_tag.text.strip()    # .text는 태그 안의 텍스트만 가져오며, .strip()은 앞뒤 불필요한 공백 제거\n",
    "        link = a_tag['href']    # 태그의 속성 중 'href'의 값(링크 주소)을 가져옴\n",
    "        print(title,link)  \n",
    "        #print(type(a_tag),a_tag) #a_tag타입과 a_tag 자체를 print로 확인\n",
    "    else:\n",
    "        # 응답(response)이 Error 이면 status code 출력    \n",
    "        print(f'Error code = {res.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_news('국제')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
